<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <meta content="width=device-width, initial-scale=1" name="viewport">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta content="" name="Lab10">
    <meta content="" name="Zhiyuan">

    <title>Starter Template for Bootstrap</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]>
    <script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>

<nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container">
        <div class="navbar-header">
            <button aria-controls="navbar" aria-expanded="false" class="navbar-toggle collapsed" data-target="#navbar"
                    data-toggle="collapse" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#">Lab10</a>
        </div>
        <div class="collapse navbar-collapse" id="navbar">
            <ul class="nav navbar-nav">
                <!--            <li class="index.html"><a href="#">Home</a></li>-->
                <!--            <li><a href="#intro">Introduction</a></li>-->
                <!--            <li><a href="#obj">Project Objective</a></li>-->
                <!--            <li><a href="#design">Procedure and Result</a></li>-->
                <!--            <li><a href="#drawings">Drawings</a></li>-->
                <!--            <li><a href="#testing">Testing</a></li>-->
                <!--            <li><a href="#result">Result</a></li>-->
            </ul>
        </div><!--/.nav-collapse -->
    </div>
</nav>

<div class="container">

    <div class="starter-template">
        <h1>Lab 10: Grid Localization using Bayes Filter </h1>
        <h4>Zhiyuan Zhang</h4>


    </div>

    <!--      <hr>-->
    <!--      <div class="center-block">-->
    <!--          <iframe width="640" height="360" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allowfullscreen></iframe>-->
    <!--          <h4 style="text-align:center;">Demonstration Video</h4>-->
    <!--      </div>-->

    <hr id="intro">

    <div style="text-align:center;">
        <h2>Introduction</h2>
        <p style="text-align: left;padding: 0px 30px;">In this lab, we applied the Bayes filter for
            grid-based localization. Our objective was to simulate a robot navigating a predetermined and preprogrammed
            path. At each time step, we employed the Bayes filter to ascertain the robot's location (localization). </p>

        <hr id='obj'>

        <!--      <div class="row">-->
        <!--          <div class="col-md-4" style="text-align:center;">-->
        <!--          <img class="img-rounded" src="pics/1.jpg" alt="Generic placeholder image" width="240" height="240">-->
        <!--          </div>-->
        <!--          <div class="col-md-8" style="font-size:18px;">-->
        <!--          <h2>Project Objective:</h2>-->
        <!--          <ul>-->
        <!--              <li>some important objectives.some important objectives.some important objectives.some important objectives.</li>-->
        <!--                <li>some other important objectives.</li>-->
        <!--            <li>some not-that-important objectives.</li>-->
        <!--          </ul>-->
        <!--          </div>-->
        <!--      </div>-->

        <!--    <hr id='design'>-->

        <div style="text-align:center;">
            <h2>Lab Procedure And Result</h2>
            <h3>Grid in Simulator</h3>
            <p style="text-align: left;padding: 0px 30px;">In this lab, robot localization refers to determining the
                robot's state (coordinates and orientation)
                after each movement. The robot's state in the simulator is defined as a 3D representation with (x, y, θ)
                coordinates. The robot's environment (grid map) encompasses a continuous space, extending from:</p>
            <ul>
                <li>[-1.6764, +1.9812) meters or [-5.5, 6.5) feet in the x-direction.</li>
                <li>[-1.3716, +1.3716) meters or [-4.5, +4.5) feet in the y-direction.</li>
                <li>[-180, +180) degrees along the theta axis.</li>
            </ul>
            <p>The virtual robot shown in the simulator image starts at the initial state of (0, 0, 0). The X-axis
                increases from left to right, the Y-axis increases from bottom to top, and the Theta-axis increases when
                the robot spins clockwise and decreases when it spins counter-clockwise.</p>

            <p style="text-align: left;padding: 0px 30px;"></p>

            <p style="text-align: left;padding: 0px 30px;"></p>
            <h3>3D Grid</h3>
            <p style="text-align: left;padding: 0px 30px;">
                The grid map is divided into uniformly sized cells. Each grid cell has dimensions of 0.3048 meters along
                the x and y axes, and 20 degrees along the θ axis. Consequently, the continuous state space is
                discretized into cells as follows:</p>
            <p style="text-align: left;padding: 0px 30px;">X-axis cells: [0, 12); with a 0.3048-meter resolution.<br>
                Y-axis cells: [0, 9); with a 0.3048-meter resolution.<br>
                Theta-axis cells: [0, 18); with a 20-degree resolution.</p>
            </p>
            <img src="pics/3dgrid.jpg" width="800">

            <h3>Bayes Filter Algorithm</h3>
            <img src="pics/bayes.PNG" width="800">
            <p style="text-align: left;padding: 0px 30px;">In this lab, the main work is to implement the byes Algorithm
                above </p>
            <h3>Implement Compute_Control</h3>

            <p style="text-align: left;padding: 0px 30px;">The First function to implement involves calculating the
                actual action the robot takes to transition from one state to another (the robot's first movement
                step).</p>
            <p style="text-align: left;padding: 0px 30px;">The action can be broken down into three stages, as
                illustrated below:</p>
            <ol>
                <li>The robot must rotate by a specific angle (delta_rot1) to align itself with the next state's
                    position.
                </li>
                <li>Next, it moves (delta_trans) to a new grid cell on the map.</li>
                <li>Finally, the robot rotates to a new angle (delta_rot2) to arrive at its intended state.</li>
            </ol>
            <img src="pics/control_formula.PNG" width="500">

            <p style="text-align: left;padding: 0px 30px;">By employing basic trigonometry, I utilized the inverse
                tangent and the difference between the current and previous poses along the x and y axes to determine
                the initial rotation. Subsequently, I applied the Pythagorean theorem and the difference between the
                current and previous poses along the x and y axes to compute the translation. Lastly, to obtain the
                third rotation, I subtracted the initial rotation and the previous pose rotation from the current pose
                and normalized the result. The code for this function is provided below:</p>
            <script src="https://gist.github.com/Zhang9340/70edd915a1a609e27030675c9c70a02c.js"></script>

            <h3>Odometry Motion Model</h3>
            <p style="text-align: left;padding: 0px 30px;">This function computes the likelihood of reaching the current
                state (cur_pose) given a control action (u)
                and the robot's previous state (prev_pose). In this case, the provided control action (u) may differ
                from the actual action calculated using the previously defined function, compute_control, which we can
                refer to as the actual action.</p>
            <p style="text-align: left;padding: 0px 30px;">To calculate the probability, a Gaussian function is used to
                model noise, with a mu representing the mean
                and a sigma representing the standard deviation.The probability can be computed as:</p>
            <img src="pics/odometry_fun.PNG" width="500">
            <p style="text-align: left;padding: 0px 30px;">The code below shows implementation of the formulas:</p>
            <script src="https://gist.github.com/Zhang9340/8800fde259f36b754071c948378198b1.js"></script>
            <h3>Prediction Step</h3>
            <p>Subsequently, I developed the prediction_step function, which corresponds to the following components of
                the Bayes Filter:</p>
            <img src="pics/prediction.png" width="500">
            <p style="text-align: left;padding: 0px 30px;">The code below shows implementation of the formulas:</p>
            <script src="https://gist.github.com/Zhang9340/313851bcaf005623768c002e2d258d52.js"></script>
            <h3>Sensor Model</h3>
            <img src="pics/sensor.PNG" width="500">
            <p style="text-align: left;padding: 0px 30px;">This function calculates the probability of accurate sensor
                distance measurements given the current
                state.
                As with the previous example, the probability is derived using a Gaussian function. The function needs
                an input, 'obs', which contains the true (accurate) distance measurements for a state. The
                sensor-measured distances can be accessed by calling the 'loc.obs_range_data' variable. Consequently,
                the probabilities for each of the 18 sensor measurements can be computed and stored in a single
                array.</p>
            <script src="https://gist.github.com/Zhang9340/34da49b8cb14f22b048ebf5c4d2c188c.js"></script>
            <h3>Update Step</h3>
            <img src="pics/update.png" width="500">

            <p style="text-align: left;padding: 0px 30px;">This function primarily updates the 'bel' matrix with the
                probability distribution representing the
                robot's estimated location based on sensor measurements and 'bel_bar'. To achieve this, I employed three
                nested for loops, one for each dimension. Within the innermost loop, I essentially translated line 3 of
                the Bayes Filter into Python code. Lastly, I normalized the results. The code for this function is
                provided below:</p>
            <script src="https://gist.github.com/Zhang9340/7e2cd3144099fce6f5066a85a62339e5.js"></script>
            <h3>Result of Bayes Filter Implementation</h3>

            <p style="text-align: left;padding: 0px 30px;">With the Bayes Filter fully developed, it's now time to
                implement and assess its performance. In the video below, the "Trajectory Plotter" displays three lines
                with distinct colors as the robot moves:</p>
            <ol>
                <li>The green line illustrates a pre-programmed trajectory used as the ground truth to test the Bayes
                    Filter.
                </li>
                <li>The red line signifies odometry readings, which are the noisiest and least accurate.</li>
                <li>The blue line represents the belief calculated by the implemented Bayes Filter algorithm, which
                    closely aligns with the ground truth line.
                </li>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/E2SfR2PdsRA"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                        allowfullscreen></iframe>
            </ol>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/WPxwnKn_8JA"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    allowfullscreen></iframe>

            <p>It is evident that the blue line (belief) closely aligns with the green line (ground truth), while the
                odom falls significantly short in accuracy. In the second video featuring white boxes, a whiter box
                indicates a higher probability of movement.</p>
            <script src="https://gist.github.com/Zhang9340/3484532496b469a73844bf36262d26f4.js"></script>
        </div>

    </div><!-- /.container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
</body>
</html>
